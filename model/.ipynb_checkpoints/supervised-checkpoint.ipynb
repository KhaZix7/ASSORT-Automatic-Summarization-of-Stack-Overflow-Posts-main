{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkV1AhYHhioH"
   },
   "source": [
    "# Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2hTxUoLgjNA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XaNDWlIh6JE"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "53cRbVRDgjNG"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import numpy as np\n",
    "import string\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "import sys\n",
    "import gensim\n",
    "from random import randrange\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Sampler\n",
    "from torchvision import datasets,transforms\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SCxTEY1dgjNJ"
   },
   "outputs": [],
   "source": [
    "def findMax(result, threshold):\n",
    "    index = 0\n",
    "    if result[1] - result[0] > 0:\n",
    "        index = 1\n",
    "    return index\n",
    "\n",
    "def findAccuracy(solution, fact):\n",
    "    correct = 0\n",
    "    for index, answer in enumerate(solution):\n",
    "        if fact[index] == answer:\n",
    "            correct += 1\n",
    "    return float(correct) / len(solution)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, dataLen, truth):\n",
    "        self.dataLen = dataLen\n",
    "        self.truth = torch.tensor(truth, dtype=torch.long)\n",
    "        self.data = torch.tensor(data, dtype=torch.double)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.data[index], self.truth[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataLen\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        if size < 700:\n",
    "            self.fc1 = nn.Linear(size, size)\n",
    "            self.fc2 = nn.Linear(size, size)\n",
    "            self.fc3 = nn.Linear(size, 2)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(size, 450)\n",
    "            self.fc2 = nn.Linear(450, 15)\n",
    "            self.fc3 = nn.Linear(15, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))\n",
    "\n",
    "def train_nn(trainSet, trainTruth, testSet, testTruth, qtype, lr, epoch):\n",
    "    learning_rate = lr\n",
    "    epochs = epoch\n",
    "    batch_size = 256\n",
    "    model_name = \"nn_old_approach_50_\" + qtype +\"_best.pt\"\n",
    "\n",
    "    trainSet = torch.from_numpy(trainSet).double()\n",
    "    train_dataLen = len(trainSet)\n",
    "    test_dataLen = len(testSet)\n",
    "    trainAccuracies = []\n",
    "    testAccuracies = []\n",
    "    epochLists = []\n",
    "\n",
    "    net = Net(len(trainSet[0]))\n",
    "    optimizer = optim.Adam(net.parameters(), lr = learning_rate,  weight_decay=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    trainDataset = Dataset(data = trainSet, dataLen = train_dataLen, truth = trainTruth)\n",
    "    trainLoader = DataLoader(trainDataset, batch_size = batch_size)\n",
    "    testDataset = Dataset(data = testSet, dataLen = test_dataLen, truth = testTruth)\n",
    "    testLoader = DataLoader(testDataset, batch_size = batch_size)\n",
    "\n",
    "    for i in trange(epochs):\n",
    "        for train, target in trainLoader:\n",
    "            optimizer.zero_grad()\n",
    "            score = net(train.float())\n",
    "            loss = criterion(score, target.view(len(target)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if i % 5 == 0 or i == epochs - 1:\n",
    "            for threshold in [-0.2]:\n",
    "                train_solution = []\n",
    "                for j in trainSet:\n",
    "                    j = j.float()\n",
    "                    score = net(j).tolist()\n",
    "                    oneResult = findMax(score, threshold)\n",
    "                    train_solution.append(oneResult)\n",
    "                train_accuracy = accuracy_score(trainTruth, train_solution)\n",
    "                train_f1 = f1_score(trainTruth, train_solution)\n",
    "                train_precision = precision_score(trainTruth, train_solution)\n",
    "                train_recall = recall_score(trainTruth, train_solution)\n",
    "\n",
    "                test_solution = []\n",
    "                for j in testSet:\n",
    "                    j = torch.tensor(j, dtype=torch.float)\n",
    "                    score = net(j).tolist()\n",
    "                    oneResult = findMax(score, threshold)\n",
    "                    test_solution.append(oneResult)\n",
    "                # test_accuracy = findAccuracy(test_solution, testTruth)\n",
    "                test_accuracy = accuracy_score(testTruth, test_solution)\n",
    "                test_f1 = f1_score(testTruth, test_solution)\n",
    "                test_precision = precision_score(testTruth, test_solution)\n",
    "                test_recall = recall_score(testTruth, test_solution)\n",
    "                \n",
    "                trainAccuracies.append(train_accuracy)\n",
    "                testAccuracies.append(test_accuracy)\n",
    "                epochLists.append(i)\n",
    "                torch.save(net.state_dict(), model_name)\n",
    "\n",
    "    print(\"Model stored to: {}\".format(model_name))\n",
    "    torch.save(net.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mesdp_2hpFd"
   },
   "source": [
    "# Feature calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zNbkU1BqgjNM"
   },
   "outputs": [],
   "source": [
    "# Domain feature calculation\n",
    "def entity_overlap(sentence, tags, question_title):\n",
    "  tags = literal_eval(tags)\n",
    "  total = len(tags)\n",
    "  overlap = 0\n",
    "  for i in tags:\n",
    "    if question_title.lower().find(i) != -1 and sentence.lower().find(i) != -1:\n",
    "      overlap += 1\n",
    "  if total != 0:\n",
    "    return float(overlap) / total\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def comparative_adjective(tags):\n",
    "  result = 0\n",
    "  for x, y in tags:\n",
    "    if y == \"JJR\":\n",
    "      result = 1\n",
    "  return result\n",
    "\n",
    "def superlative_adjective(tags):\n",
    "  result = 0\n",
    "  for x, y in tags:\n",
    "    if y == \"JJS\":\n",
    "      result = 1\n",
    "  return result\n",
    "\n",
    "def imperative_sentence(tags):\n",
    "  if tags[0][1] == \"VB\":\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def pattern_matching(sentence):\n",
    "  sentence = sentence.lower()\n",
    "  result = [0] * 19\n",
    "  patterns = [\"however,\", 'first,', 'in short,', 'in this case,', 'in general,', 'on the other hand,', 'then,', \\\n",
    "              'alternatively,', 'in other words,', 'in addition,', 'in practice,', 'in fact,', 'otherwise,', 'if you care,',\n",
    "              'in contrast,', 'finally,', 'below is', 'additionally,', 'furthermore,']\n",
    "  for index, i in enumerate(patterns):\n",
    "    if sentence.find(i) != -1:\n",
    "      result[index] = 1\n",
    "  return result\n",
    "\n",
    "def explicit_features(i, index, tags, question_title):\n",
    "  results = []\n",
    "  truth = []\n",
    "  ef_for_one_sentence = [0] * 9\n",
    "  ef_for_one_sentence[0] = entity_overlap(i, tags, question_title)\n",
    "  if index < 3:\n",
    "    ef_for_one_sentence[1] = 1 \n",
    "  if i.find(\"<code\") != 0:\n",
    "    ef_for_one_sentence[3] = 1\n",
    "  if i.find(\"<br\") != 0:\n",
    "    ef_for_one_sentence[4] = 1\n",
    "  if i.find('<li') != 0:\n",
    "    ef_for_one_sentence[5] = 1\n",
    "  \n",
    "  # Comparative/Superlative adjectives, imperative sentence\n",
    "  words = nltk.word_tokenize(i)\n",
    "  tagged = nltk.pos_tag(words)\n",
    "  ef_for_one_sentence[6] = comparative_adjective(tagged)\n",
    "  ef_for_one_sentence[7] = superlative_adjective(tagged)\n",
    "  ef_for_one_sentence[8] = imperative_sentence(tagged)\n",
    "  #Phrase matching\n",
    "  ef_for_one_sentence += pattern_matching(i)\n",
    "  return ef_for_one_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8cw7MKVkgjNN"
   },
   "outputs": [],
   "source": [
    "# Function to get sentence representation\n",
    "def get_representation(df, tokenizer, model):\n",
    "  explicit_features_for_df = np.empty((0, 28), dtype=float, order='C')\n",
    "  for index, i in enumerate(df.sentence):\n",
    "    position=df.position[index]\n",
    "    tags = df.tags[index]\n",
    "    question_title = df.question_title[index]\n",
    "    explicit_features_for_df = np.append(explicit_features_for_df, np.array(explicit_features(i, position, tags, question_title)))\n",
    "  explicit_features_for_df = explicit_features_for_df.reshape(-1, 28)\n",
    "  \n",
    "  sentence_bert_for_df = np.empty((0, 768), dtype=float, order='C')\n",
    "\n",
    "  for i in trange(len(df.sentence)):\n",
    "    i = df.sentence[i]\n",
    "    encoded_input = tokenizer(i, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    sentence_bert_for_df = np.append(sentence_bert_for_df, output['pooler_output'].detach().numpy(), axis = 0)\n",
    "  \n",
    "  return np.concatenate((sentence_bert_for_df, explicit_features_for_df), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muKLQPPEhs4U"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XDe65I-S3CcK"
   },
   "outputs": [],
   "source": [
    "def get_categorized_representation(df, representation):\n",
    "  result = dict()\n",
    "  how_to_index = df.loc[df['question_type'] == 2].index\n",
    "  conceptual_index = df.loc[df['question_type'] == 1].index\n",
    "  debug_index = df.loc[df['question_type'] == 3].index\n",
    "  result[\"how_to\"] = representation[[how_to_index]]\n",
    "  result[\"conceptual\"] = representation[[conceptual_index]]\n",
    "  result[\"debug\"] = representation[[debug_index]]\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofFIfqePgjNO"
   },
   "outputs": [],
   "source": [
    "# BERTOverflow\n",
    "boverflow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "boverflow_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "old_boverflow_embeddings = get_representation(dataset, boverflow_tokenizer, boverflow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5p30SW4itr7"
   },
   "outputs": [],
   "source": [
    "pickle.dump(old_boverflow_embeddings, open(\"net_embeddings.txt\", \"wb\"))\n",
    "old_boverflow_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgu_umiQ3FJZ"
   },
   "outputs": [],
   "source": [
    "training_data = get_categorized_representation(dataset, old_boverflow_embeddings)\n",
    "pickle.dump(training_data, open(\"training_data.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owsLttE13jrp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_conceptual, X_test_conceptual, y_train_conceptual, y_test_conceptual = train_test_split(training_data[\"conceptual\"], list(dataset.loc[dataset['question_type'] == 1].truth), test_size=0.1, random_state=42)\n",
    "train_nn(X_train_conceptual, y_train_conceptual, X_test_conceptual, y_test_conceptual, \"conceptual\", lr=1e-5, epoch=150)\n",
    "\n",
    "# Train how_to sentence classifier\n",
    "X_train_howto, X_test_howto, y_train_howto, y_test_howto = train_test_split(training_data[\"howto\"], list(dataset.loc[dataset['question_type'] == 2].truth), test_size=0.1, random_state=42)\n",
    "train_nn(X_train_howto, y_train_howto, X_test_howto, y_test_howto, \"howto\", lr=1e-5, epoch=150)\n",
    "\n",
    "# Train debug sentence classifier\n",
    "X_train_debug, X_test_debug, y_train_debug, y_test_debug = train_test_split(training_data[\"debug\"], list(dataset.loc[dataset['question_type'] == 3].truth), test_size=0.1, random_state=42)\n",
    "train_nn(X_train_debug, y_train_debug, X_test_debug, y_test_debug, \"debug\", lr=1e-5, epoch=150)\n",
    "\n",
    "# Constructing test_set\n",
    "test_set = np.concatenate((y_train_conceptual, y_train_howto, y_train_debug), axis = 0)\n",
    "test_truth = np.concatenate((y_test_conceptual, y_test_howto, y_test_debug), axis = 0)\n",
    "\n",
    "vectorizer = pickle.load(open('vectorizer.sav', 'rb'))\n",
    "clf = pickle.load(open('question_classifier.sav', 'rb'))\n",
    "\n",
    "print(\"Getting bag-of-word representations of question titles\")\n",
    "new_bag_of_words = vectorizer.transform(dataset.question_title).toarray()\n",
    "probs = clf.predict_proba(new_bag_of_words)\n",
    "\n",
    "print(\"Loading sentence-classifiers we just trained...\")\n",
    "# Howto model\n",
    "size = 796\n",
    "howtomodel = Net(size)\n",
    "howtomodel.load_state_dict(torch.load('nn_old_approach_50_how_to_best.pt'))\n",
    "# Conceptual model\n",
    "conceptualmodel = Net(size)\n",
    "conceptualmodel.load_state_dict(torch.load('nn_old_approach_50_conceptual_best.pt'))\n",
    "# Debug model\n",
    "debugmodel = Net(size)\n",
    "debugmodel.load_state_dict(torch.load('nn_old_approach_50_debug_best.pt'))\n",
    "\n",
    "print(\"Getting bag-of-word representations of question titles\")\n",
    "new_bag_of_words = vectorizer.transform(dataset.question_title).toarray()\n",
    "probs = clf.predict_proba(new_bag_of_words)\n",
    "\n",
    "print(\"Applying different sentence classifiers...\")\n",
    "def get_sm_from_sentence_classifier(model, embeddings):\n",
    "  m = nn.Softmax(dim=1)\n",
    "  scores = model(torch.tensor(embeddings).float())\n",
    "  return m(scores).detach().numpy()[:, 1]\n",
    "\n",
    "new_howtosm = get_sm_from_sentence_classifier(howtomodel, test_set)\n",
    "new_conceptualsm = get_sm_from_sentence_classifier(conceptualmodel, test_set)\n",
    "new_debugsm = get_sm_from_sentence_classifier(debugmodel, test_set)\n",
    "\n",
    "new_scores_from_sentence_classifier = np.concatenate((new_howtosm.reshape(1, -1), new_conceptualsm.reshape(1, -1), new_debugsm.reshape(1, -1)), axis = 0)\n",
    "\n",
    "new_final_score = probs.T * new_scores_from_sentence_classifier\n",
    "\n",
    "new_final_score = new_final_score[0] + new_final_score[1] + new_final_score[2]\n",
    "\n",
    "print(\"With threshold 0.5, calculating model performance...\")\n",
    "new_truth = [1 if i > 0.1 else 0 for i in new_final_score]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "precision = precision_score(dataset.truth.tolist(), new_truth)\n",
    "recall = recall_score(dataset.truth.tolist(), new_truth)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(\"precision:\", precision)\n",
    "print(\"recall:\", recall)\n",
    "print(\"F1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
